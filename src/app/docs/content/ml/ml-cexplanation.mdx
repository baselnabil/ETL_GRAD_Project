---
title: Machine Learning Code Explanation
description: Step-by-step breakdown of the machine learning model code used to predict experience level based on job features.
---

# Code Overview

### Introduction to Model Selection
In this section, we break down the code behind the implementation of the salary prediction model. A Random Forest Regressor is used due to its flexibility and ability to handle both numerical and categorical variables. This model is particularly effective for datasets with complex feature interactions, like the one we are working with, which includes job titles, experience levels, employment types, and other salary-related factors.

The following key elements are covered:

- **Data Preprocessing**: Handling missing values and encoding categorical features.
- **Model Training**: Using the Random Forest Regressor for salary prediction.
- **Model Evaluation**: Assessing the model performance using R-squared and Mean Squared Error (MSE).

---

# Code Explanation

## 1. Data Preprocessing

```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder

# Load the data
data = pd.read_csv('data_engineer_salaries.csv')

# Handling missing values
data = data.dropna()

# One-hot encoding categorical variables
categorical_columns = ['experience_level', 'employment_type', 'job_title', 'company_location']
encoder = OneHotEncoder(sparse=False)
encoded_data = encoder.fit_transform(data[categorical_columns])

# Creating the feature matrix (X) and target variable (y)
X = pd.concat([pd.DataFrame(encoded_data), data[['work_year', 'salary_in_usd']]], axis=1)
y = data['salary_in_usd']

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

### Explanation:

- **Data Loading and Handling**: We start by loading the CSV file containing salary data and dropping any rows with missing values to ensure clean input.
- **Categorical Encoding**: We apply one-hot encoding to categorical columns like `experience_level`, `employment_type`, `job_title`, and `company_location` to convert them into a format suitable for model training.
- **Feature Matrix and Target Variable**: The encoded categorical data is combined with numerical features like `work_year` and `salary_in_usd` to form the feature matrix (X), with `salary_in_usd` as the target variable (y).
- **Train-Test Split**: The dataset is split into training and test sets using an 80/20 split to evaluate the model’s performance.
<hr />
<br />

## 2. Model Training and Evaluation

```python
# Initialize the Random Forest Regressor
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)

# Train the model
rf_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = rf_model.predict(X_test)

# Evaluate the model's performance
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")
```

### Explanation:

- **Model Initialization**: We initialize the `RandomForestRegressor` with 100 trees (n_estimators=100). Random Forest is chosen for its ability to model complex relationships and handle various feature types without heavy preprocessing.
- **Model Training**: The model is trained using the `fit()` method, where it learns from the training set (X_train, y_train) to predict salary values.
- **Predictions**: Once trained, the model generates predictions for the test set (X_test), which are then compared against the actual values (y_test).
- **Model Evaluation**: Two evaluation metrics are used:
  - **Mean Squared Error (MSE)**: Measures the average squared difference between predicted and actual values. Lower values indicate better model performance.
  - **R-squared (R²)**: Represents the proportion of variance in the target variable (salary_in_usd) that is predictable from the feature set. Values closer to 1 indicate a better fit.
<hr />
<br />

## 3. Additional Data Processing and Model Retraining

```python
import re
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

t2_df = pd.read_csv('DataEngineer.csv')
t2New_df = pd.DataFrame()

# Function to Extract Salary
def extract_salary(salary_str):
    match = re.search(r'$(\d+)K-$(\d+)K', salary_str)
    if match:
        lower = int(match.group(1)) * 1000
        upper = int(match.group(2)) * 1000
        return int((lower + upper) / 2)
    return None

# Function to Extract Work Year Ratio
def extract_work_year_ratio(df):
    year_counts = df['work_year'].value_counts(normalize=True)
    return year_counts

# Generate Work Years Based on Year Ratio
def generate_work_years(n, year_ratio):
    years = np.random.choice(year_ratio.index, size=n, p=year_ratio.values)
    return years

year_ratio = extract_work_year_ratio(data)

# Map Company Size
def map_company_size(size_str):
    if '1 to 50 employees' in size_str or '51 to 200 employees' in size_str:
        return 'S'
    elif '201 to 500 employees' in size_str or '501 to 1000 employees' in size_str:
        return 'M'
    elif '1001 to 5000 employees' in size_str or '5001 to 10000 employees' in size_str or '10000+ employees' in size_str:
        return 'L'
    return 'NA'

# Process new data
t2New_df_dict = {
    'work_year': generate_work_years(len(t2_df), year_ratio),
    'employment_type': 'FT',  # placeholder. Full Time
    'job_title': t2_df['Job Title'],
    'salary': t2_df['Salary Estimate'].apply(extract_salary),
    'salary_currency': 'USD',
    'salary_in_usd': t2_df['Salary Estimate'].apply(extract_salary),
    'employee_residence': t2_df['Location'],
    'remote_ratio': 0,
    'company_location': t2_df['Location'],
    'company_size': t2_df['Size'].apply(map_company_size)
}

t2New_df = pd.DataFrame(t2New_df_dict)
t2New_df_processed = pd.get_dummies(t2New_df, drop_first=True)

# Ensure the columns match between the data and t2New_df
missing_cols = set(X.columns) - set(t2New_df_processed.columns)
missing_cols = list(missing_cols)

# Create a DataFrame with all zeros for the missing columns
missing_df = pd.DataFrame(0, index=t2New_df_processed.index, columns=missing_cols)

# Concatenate the new columns with the existing DataFrame
t2New_df_processed = pd.concat([t2New_df_processed, missing_df], axis=1)

# Reorder columns to match the original DataFrame X
t2New_df_processed = t2New_df_processed[X.columns]

# Predict Experience Levels
t2New_df['experience_level'] = best_rf_model.predict(t2New_df_processed)

# Retrain the model on the combined dataset
rf_model.fit(t2New_df_processed, t2New_df['experience_level'])

# Evaluate the model again
y_pred_retrained = best_rf_model.predict(X_test)
retrained_accuracy = accuracy_score(y_test, y_pred_retrained)
print(f'Retrained accuracy: {retrained_accuracy:.2f}')
print(classification_report(y_test, y_pred_retrained))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred_retrained)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - Retrained Model')
plt.show()
```

### Explanation:

- **Additional Data Processing**: We load new data, extract salary values, and generate work years based on the year ratio. We also map company sizes and process the new data to match the original feature set.
- **Model Retraining**: The model is retrained on the combined dataset, and predictions are made on the test set.
- **Model Evaluation**: The retrained model's accuracy is evaluated, and a confusion matrix is generated to visualize the performance.

<hr />

## 4. Conclusion

The **Random Forest Regressor** was chosen for its robustness and ability to handle complex datasets like the salary data, which includes both categorical and numerical features. By carefully preprocessing the data and training the model, we were able to predict experience level with a reasonable degree of accuracy. This approach can be further improved by fine-tuning hyperparameters or experimenting with different models, but the current setup provides a solid foundation for salary prediction.