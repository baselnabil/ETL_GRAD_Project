---
title: ETL - Loading
description: Overview of the data loading process in GradSync
---

The final step in the ETL process is loading the transformed data into the target databases. In GradSync, we utilize two database systems for different purposes: PostgreSQL as a staging database and MariaDB as an OLAP (Online Analytical Processing) database. This dual-database approach ensures efficient data storage, retrieval, and analysis.

<br />

```sql
use main;
CREATE TABLE date_dim (
    date_id INT AUTO_INCREMENT PRIMARY KEY,
    work_year INT NOT NULL);
 SELECT * from job_dim;
 
-- Job Dimension
CREATE TABLE job_dim (
    job_id INT AUTO_INCREMENT PRIMARY KEY,
    job_title VARCHAR(100) NOT NULL,
    experience_level VARCHAR(50) NOT NULL,
    employment_type VARCHAR(50) NOT NULL
);
INSERT into job_dim (job_title, experience_level, employment_type) VALUES ('Data Scientist', 'SE', 'FT');
-- Employee Dimension
CREATE TABLE employee_dim (
    employee_id INT AUTO_INCREMENT PRIMARY KEY,
    employee_residence VARCHAR(100) NOT NULL
);
SELECT * FROM employee_dim;
-- Company Dimension
CREATE TABLE company_dim (
    company_id INT AUTO_INCREMENT PRIMARY KEY,
    company_location VARCHAR(100) NOT NULL,
    company_size VARCHAR(20) NOT NULL,
    remote_ratio INT NOT NULL
);
```

<br />



```python
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, IntegerType, StringType



def extract_data():
    from pyspark.sql import SparkSession
    from pyspark.sql.types import StructType, StructField, IntegerType, StringType
    schema = StructType([
    StructField("work_year", IntegerType(), True),
    StructField("experience_level", StringType(), True),
    StructField("employment_type", StringType(), True),
    StructField("job_title", StringType(), True),
    StructField("salary", IntegerType(), True),
    StructField("salary_currency", StringType(), True),
    StructField("salary_in_usd", IntegerType(), True),
    StructField("employee_residence", StringType(), True),
    StructField("remote_ratio", IntegerType(), True),
    StructField("company_location", StringType(), True),
    StructField("company_size", StringType(), True)
])
    spark=SparkSession.builder \
        .appName('extractor') \
        .getOrCreate()
    

    data1 = spark.read.csv('/opt/airflow/data/raw/salaries_2.csv', header=True, schema=schema)
    data2 = spark.read.csv('/opt/airflow/data/raw/jobs_in_data.csv', header=True, inferSchema=True)
    data3 = spark.read.csv('/opt/airflow/data/raw/jobs.csv', header=True, inferSchema=True)
    data2 = data2.select(
    "work_year",
    "experience_level",
    "employment_type",
    "job_title",
    "salary",
    "salary_currency",
    "salary_in_usd",
    "employee_residence",
    "work_setting",
    "company_location",
    "company_size"
    ).withColumnRenamed("work_setting", "remote_ratio")

    data3 = data3.select(
    data3["date_posted"].cast(IntegerType()).alias("work_year"),
    data3["job_level"].alias("experience_level"),
    data3["job_type"].alias("employment_type"),
    data3["title"].alias("job_title"),
    data3["min_amount"].cast(IntegerType()).alias("salary"),
    data3["currency"].alias("salary_currency"),
    data3["max_amount"].cast(IntegerType()).alias("salary_in_usd"),
    data3["location"].alias("employee_residence"),
    data3["is_remote"].cast(IntegerType()).alias("remote_ratio"),
    data3["location"].alias("company_location"),
    data3["company_num_employees"].alias("company_size")
)
    df_parquet= spark.read.parquet('/opt/airflow/data/raw/data.parquet')    
    merged_dataset = data1.union(data2).union(data3).union(df_parquet)
    merged_dataset.coalesce(1).write.parquet("/opt/airflow/data/processed", mode='overwrite')
```

<br />
### PostgreSQL (Staging Database)

PostgreSQL serves as our staging area for initial data loading. It provides a robust and reliable environment for temporarily storing data before it is moved to the OLAP database. Here are the key aspects of using PostgreSQL as a staging database:

- **Initial Data Loading**: We use Pandas to populate data into the staging area. Pandas' DataFrame operations make it easy to manipulate and load data into PostgreSQL.
- **Timestamp Tracking**: A timestamp column is added to each table to track data loading times. This helps in monitoring and auditing the data loading process.
- **Schema Inference and Overwriting**: SQL commands are used for schema inference and overwriting. This ensures that the data conforms to the expected schema and allows for efficient updates.
- **Historical Data Archiving**: An archive table is maintained for historical data. This allows us to keep track of changes over time and perform historical analysis.

<br />

### MariaDB (OLAP Database)

MariaDB is chosen as our OLAP database due to its columnar storage capabilities, which offer several advantages for analytical workloads. Here are the key benefits of using MariaDB as an OLAP database:

- **Improved Query Performance**: Columnar storage allows for faster analytical queries by reading only the necessary columns. This reduces the amount of data read from disk and speeds up query execution.
- **Better Compression**: Data in columns often has similar values, leading to better compression ratios. This reduces storage requirements and improves I/O performance.
- **Parallel Processing**: Columnar databases can easily parallelize operations on individual columns. This enhances the performance of complex analytical queries that involve aggregations and calculations on specific columns across many rows.

<br />

### Columnar Storage Benefits

Columnar databases store data by column rather than by row, which is particularly beneficial for analytical workloads. Here are some additional benefits of columnar storage:

- **Efficient Aggregations**: Columnar storage is optimized for aggregations and calculations on specific columns. This makes it ideal for OLAP workloads that involve summarizing large datasets.
- **Reduced I/O**: By reading only the necessary columns, columnar storage reduces the amount of data read from disk. This improves query performance and reduces I/O bottlenecks.
- **Data Compression**: Columnar storage achieves better compression ratios by storing similar values together. This reduces storage costs and improves query performance.

<br />

## Conclusion

The loading process in GradSync involves populating data into PostgreSQL as a staging database and MariaDB as an OLAP database. By leveraging the strengths of both databases, we ensure efficient data storage, retrieval, and analysis. PostgreSQL provides a reliable staging area for initial data loading, while MariaDB's columnar storage capabilities enhance the performance of analytical queries. This dual-database approach enables us to deliver accurate and timely insights from our data.