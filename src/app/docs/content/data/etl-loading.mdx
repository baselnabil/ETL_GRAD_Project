---
title: ETL - Loading
description: Overview of the data loading process in GradSync
---
# Code Overview

The final step in the ETL process is loading the transformed data into the target databases. In GradSync, we utilize two database systems for different purposes: PostgreSQL as a staging database and MariaDB as an OLAP (Online Analytical Processing) database. This dual-database approach ensures efficient data storage, retrieval, and analysis.

<br />

### PostgreSQL (Staging Database)

PostgreSQL serves as our staging area for initial data loading. It provides a robust and reliable environment for temporarily storing data before it is moved to the OLAP database. Here are the key aspects of using PostgreSQL as a staging database:

- **Initial Data Loading**: We use Pandas to populate data into the staging area. Pandas' DataFrame operations make it easy to manipulate and load data into PostgreSQL.
- **Timestamp Tracking**: A timestamp column is added to each table to track data loading times. This helps in monitoring and auditing the data loading process.
- **Schema Inference and Overwriting**: SQL commands are used for schema inference and overwriting. This ensures that the data conforms to the expected schema and allows for efficient updates.
- **Historical Data Archiving**: An archive table is maintained for historical data. This allows us to keep track of changes over time and perform historical analysis.

<br />
<hr />
### MariaDB (OLAP Database)

MariaDB is chosen as our OLAP database due to its columnar storage capabilities, which offer several advantages for analytical workloads. Here are the key benefits of using MariaDB as an OLAP database:

- **Improved Query Performance**: Columnar storage allows for faster analytical queries by reading only the necessary columns. This reduces the amount of data read from disk and speeds up query execution.
- **Better Compression**: Data in columns often has similar values, leading to better compression ratios. This reduces storage requirements and improves I/O performance.
- **Parallel Processing**: Columnar databases can easily parallelize operations on individual columns. This enhances the performance of complex analytical queries that involve aggregations and calculations on specific columns across many rows.

<br />

### Columnar Storage Benefits

Columnar databases store data by column rather than by row, which is particularly beneficial for analytical workloads. Here are some additional benefits of columnar storage:

- **Efficient Aggregations**: Columnar storage is optimized for aggregations and calculations on specific columns. This makes it ideal for OLAP workloads that involve summarizing large datasets.
- **Reduced I/O**: By reading only the necessary columns, columnar storage reduces the amount of data read from disk. This improves query performance and reduces I/O bottlenecks.
- **Data Compression**: Columnar storage achieves better compression ratios by storing similar values together. This reduces storage costs and improves query performance.
<hr />
<br />

# Code Explanation

### PostgreSQL Loader

The `postgresql_loader` function handles loading transformed data from a CSV file into a PostgreSQL staging table. This function also adds a `load_time` column to track when the data was processed.


```python
def postgresql_loader():
    from pyspark.sql import SparkSession
    from pyspark.sql.types import IntegerType, BooleanType, DateType, StringType, DecimalType, TimestampType, StructType, StructField
    from pyspark.sql.functions import year, col, to_timestamp
    import pandas as pd 

    psql_connection_properties = {
        "user": "airflow",
        "password": "airflow",
        "driver": "org.postgresql.Driver"
    }

    try:
        spark = SparkSession.builder \
                .appName('postgresqlloader') \
                .config('spark.jars','/opt/spark/jars/postgresql-42.6.0.jar') \
                .getOrCreate()
```

1. **Spark Session**: The function begins by initializing a Spark session using `SparkSession.builder`. This session is configured to include the PostgreSQL JDBC driver for connecting to PostgreSQL.
2. **PostgreSQL Connection Properties**: A dictionary `psql_connection_properties` is defined with credentials to connect to the PostgreSQL database.

<hr />
<br />

### Defining Schema and Reading Data

The function defines the schema to match the structure of the PostgreSQL table and reads the data from a CSV file into a DataFrame.

```python
# Define the schema to match your PostgreSQL table
schema = StructType([
    StructField("work_year", IntegerType(), True),
    StructField("experience_level", StringType(), True),
    StructField("employment_type", StringType(), True),
    StructField("job_title", StringType(), True),
    StructField("salary", DecimalType(10,2), True),
    StructField("salary_currency", StringType(), True),
    StructField("salary_in_usd", DecimalType(10,2), True),
    StructField("employee_residence", StringType(), True),
    StructField("remote_ratio", IntegerType(), True),
    StructField("company_location", StringType(), True),
    StructField("company_size", StringType(), True)
])

# Read CSV with the defined schema
df = spark.read.csv('/opt/airflow/data/final/final.csv', header=True, schema=schema)
```

1. **Schema Definition**: The schema is defined using `StructType` to ensure the DataFrame matches the PostgreSQL table structure, specifying the data types for each field.
2. **Reading CSV**: The data is read from a CSV file using the defined schema. This step ensures that the data is structured and ready for loading into PostgreSQL.

<hr />
<br />

### Adding a Timestamp and Loading Data

The function adds a `load_time` column and writes the DataFrame to the PostgreSQL database.

```python
# Add load_time column
df = df.withColumn("load_time", to_timestamp(col("work_year").cast(StringType()), "yyyy"))

try:
    df.write.jdbc(
        url="jdbc:postgresql://postgres:5432/airflow",
        table='public.staging_data',
        mode='append',
        properties=psql_connection_properties
    )
    print("Data successfully loaded to PostgreSQL")
except Exception as e:
    print(f"Error while writing to PostgreSQL: {e}")
```

1. **Timestamp Column**: A `load_time` column is added to track the year when the data was loaded. This column is derived from the `work_year` field.
2. **Loading to PostgreSQL**: The data is written to the `staging_data` table in PostgreSQL using the JDBC connector. The `mode='append'` ensures new data is added without overwriting existing records.

<hr />
<br />

### MariaDB Loader

The `mariadb_loader` function retrieves the staged data from PostgreSQL, creates dimension tables, and loads them into MariaDB.

```python
def mariadb_loader():
    from pyspark.sql import SparkSession
    from pyspark.sql.types import IntegerType, BooleanType, DateType, StringType
    from pyspark.sql.functions import year, col
    import pandas as pd

    psql_connection_properties = {
        "user": "airflow",
        "password": "airflow",
        "driver": "org.postgresql.Driver"
    }
    maria_connection_properties = {
        "user": "root",
        "password": "root",
        "driver": "org.mariadb.jdbc.Driver"
    } 
```

1. **MariaDB Connection Properties**: A new dictionary `maria_connection_properties` is defined to store credentials for connecting to the MariaDB database.
2. **Dual Database Configuration**: Both PostgreSQL and MariaDB JDBC drivers are configured, allowing the function to retrieve data from PostgreSQL and load it into MariaDB.

<hr />
<br />

### Loading Dimension Tables

The function loads specific columns (dimensions) from PostgreSQL into individual dimension tables in MariaDB.


```python
df = spark.read.jdbc(
    url="jdbc:postgresql://localhost:5423/airflow",
    table='public.staging_data',
    properties=psql_connection_properties
)

job_dim = df.select(
    col("job_title").cast(StringType()),
    col("experience_level").cast(StringType()),
    col("employment_type").cast(StringType())
)

employee_dim = df.select(col("employee_residence").cast(StringType()))

company_dim = df.select(
    col("company_location").cast(StringType()),
    col("company_size").cast(StringType()),
    col("remote_ratio").cast(IntegerType())
)

currency_dim = df.select(col("salary_currency").cast(StringType()))
```

1. **Reading from PostgreSQL**: The staged data is read from the `staging_data` table in PostgreSQL using JDBC.
2. **Dimension Tables**: The DataFrame is split into multiple dimension tables (`job_dim`, `employee_dim`, `company_dim`, and `currency_dim`) by selecting relevant columns and casting them to the appropriate types.

<hr />
<br />

### Writing to MariaDB

Each dimension table is written to MariaDB using the JDBC connector.

```python
tables = {
    'job_dim': job_dim,
    'employee_dim': employee_dim,
    'company_dim': company_dim,
    'currency_dim': currency_dim
}

for table_name, data in tables.items():
    try:
        data.write.jdbc(
            url="jdbc:mariadb://127.0.0.1:3306/main",
            table=table_name,
            mode='append',
            properties=maria_connection_properties
        )
    except Exception as e:
        print(f"Error while writing {table_name} to MariaDB: {e}")
```

1. **Writing to MariaDB**: Each dimension table is written to its corresponding table in MariaDB using the `write.jdbc` method. The `mode='append'` ensures that new data is appended to the existing tables.

---

# Conclusion

These two functions handle the final stages of the ETL process. The `postgresql_loader` function loads data into a PostgreSQL staging table, while the `mariadb_loader` function extracts data from PostgreSQL, transforms it into dimension tables, and loads it into MariaDB. By leveraging both databases, this approach ensures that data is efficiently staged, transformed, and stored for analytical processing.